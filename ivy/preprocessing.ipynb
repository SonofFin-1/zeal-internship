{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdb8ce-1634-426a-8bdd-453ce8f40cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install packages\n",
    "%pip uninstall openai --yes\n",
    "%pip install openai==0.28\n",
    "%pip install tiktoken\n",
    "%pip install pdfminer\n",
    "%pip install pdfminer.six\n",
    "%pip install docx2txt\n",
    "%pip install python-pptx\n",
    "# %pip install pinecone-client==2.2.4\n",
    "%pip install pinecone\n",
    "%pip install pinecone-client --upgrade\n",
    "\n",
    "\n",
    "\n",
    "# imports\n",
    "import openai\n",
    "#from openai import OpenAI\n",
    "import os\n",
    "import tiktoken\n",
    "from tqdm.auto import tqdm     # this is our progress bar\n",
    "import pinecone\n",
    "import re\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text, extract_pages\n",
    "import docx2txt\n",
    "from pptx import Presentation\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "# constants\n",
    "GPT_MODEL = 'gpt-4o'\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "\n",
    "# this is for the input data\n",
    "PATH = './data'\n",
    "\n",
    "# these are Pinecone vars\n",
    "INDEX = 'idx'\n",
    "\n",
    "\n",
    "###### general info\n",
    "# the purpose of calculating the number of tokens is to ensure that the text chunks fit within the token limit of the model being used\n",
    "# The notebook splits the text into chunks based on a token limit of 256. \n",
    "    # so this is what decides how long each string in the res arr will be\n",
    "    # The choice of 256 tokens as the chunk size could be based on a balance between granularity and efficiency\n",
    "    # Smaller chunk sizes would result in more chunks and more embeddings to store, which could increase storage requirements and processing time\n",
    "    # Larger chunk sizes might lead to less specific embeddings and potentially miss important details within the text.\n",
    "# The size of each line depends on the original structure of the text files being processed.\n",
    "# The text-embedding-ada-002 model produces embeddings with a dimension of 1536, so the Pinecone index is created with the same dimension to ensure compatibility.\n",
    "# The batch size is set to 128 for generating embeddings using the OpenAI API.\n",
    "    # The batch size is set to 32 when uploading the embeddings and metadata to Pinecone using the upsert operation.\n",
    "    # Larger batch sizes can be more efficient in terms of API calls and processing time, but they also require more \n",
    "    # memory and may be limited by the API's constraints\n",
    "# The max_tokens parameter determines the maximum number of tokens to retrieve from the relevant chunks based on the input query.\n",
    "    # Retrieving more tokens provides more context but also increases the response size and processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33871a-c7d5-4ddf-886b-ff3cc119cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ENV = os.getenv(\"ENV\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = 'gpt-4') -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af786385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    return extract_text(path).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(path):\n",
    "    # extract text\n",
    "    return docx2txt.process(path).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pptx(path):\n",
    "    split = []\n",
    "    prs = Presentation(path)\n",
    "    print(\"----------------------\")\n",
    "    temp = ''\n",
    "    enterCount = 0\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, 'text'):\n",
    "#                 if len(shape.text) > 0:\n",
    "#                     temp += shape.text\n",
    "#                 elif enterCount > 1:\n",
    "#                     split.append(temp)\n",
    "#                     temp = ''\n",
    "#                     enterCount = 0\n",
    "#                 elif len(shape.text) == 0:\n",
    "#                     enterCount += 1\n",
    "                split.append(shape.text)\n",
    "    \n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04189e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xlsx(path):\n",
    "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(PATH)\n",
    "    \n",
    "    xml_file = os.path.join(PATH, 'xl/sharedStrings.xml')\n",
    "    \n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for elem in root.iter():\n",
    "        if elem.text:\n",
    "            result.append(elem.text.strip())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82080044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_png(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        \n",
    "        openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "        \n",
    "        messages = [\n",
    "            { 'role': 'system', 'content': 'system message here' },\n",
    "            { 'role': 'user', 'content': [\n",
    "                {'type': 'text', 'text': f'Convert the following image into a text table.'},\n",
    "                {'type': 'image_url', 'image_url': { 'url': f'data:image/png;base64,{image}'} }\n",
    "            ]}\n",
    "        ]\n",
    "        \n",
    "        functions = [\n",
    "            {\n",
    "                'name': 'write_queries',\n",
    "                'description': 'Get information from the database',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'details': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The information needed from the database',\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        result = ''\n",
    "        \n",
    "        for chunk in openai.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "            temperature=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            function_call=None,\n",
    "            stream=True\n",
    "        ):\n",
    "            if len(chunk.choices) > 0:\n",
    "                response = chunk.choices[0]\n",
    "                finish_reason = response.finish_reason\n",
    "                content = response.delta.content\n",
    "                function = response.delta.function_call\n",
    "\n",
    "                if content is not None:\n",
    "                    print(content, end='')\n",
    "                    result += content\n",
    "        \n",
    "        return result.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f888cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_files(root_path):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            all_files.append(file_path)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88364573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range(s, delimiter = '.'):\n",
    "    parts = s.split(delimiter)\n",
    "    return delimiter.join(parts[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859f9d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "files = collect_all_files(PATH)\n",
    "\n",
    "lines = []\n",
    "for file in files:\n",
    "    lines = []\n",
    "    print(file)\n",
    "    suffix = get_range(file)\n",
    "    if suffix == 'txt':\n",
    "        lines = read_txt(file)\n",
    "    elif suffix == 'pdf':\n",
    "        lines = read_pdf(file)\n",
    "    elif suffix == 'docx':\n",
    "        lines = read_docx(file)\n",
    "    elif suffix == 'pptx':\n",
    "        lines = read_pptx(file)\n",
    "    elif suffix == 'xlsx':\n",
    "        lines = read_xlsx(file)\n",
    "    elif suffix == 'png':\n",
    "        lines = read_png(file)\n",
    "\n",
    "    temp_string = []\n",
    "    # print(lines)\n",
    "    for line in lines:\n",
    "        if num_tokens('\\n'.join(temp_string)) > 256:\n",
    "            # print('\\n'.join(temp_string))\n",
    "            messages.append('\\n'.join(temp_string))\n",
    "            temp_string = []\n",
    "        else:\n",
    "            temp_string.append(line)\n",
    "    if num_tokens('\\n'.join(temp_string)) < 256:\n",
    "        # print('\\n'.join(temp_string))\n",
    "        messages.append('\\n'.join(temp_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [message for message in messages if len(message) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fd5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not messages: \n",
    "    # test out messages\n",
    "    messages = [\"this is message 1\", \"this is message 2\", \"this is message 3\", \"this is message 4\", \"meow meow\", \"woof woof\", \"I am a cat\"]\n",
    "\n",
    "# print(messages)\n",
    "# for i in range(len(messages)):\n",
    "#     print(len(messages[i]))\n",
    "#     print(messages[i])\n",
    "#     print(\" \\n###########################\\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b013bc-e1c8-4fd8-ae89-15c0f6966eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, PodSpec\n",
    "\n",
    "INDEX = 'idx'\n",
    "\n",
    "# pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "# print(f\"{os.environ['PINECONE_API_KEY']}\")\n",
    "\n",
    "if INDEX not in pc.list_indexes().names():\n",
    "    pc.create_index(INDEX, dimension=1536, metric='cosine', spec=PodSpec(environment=ENV))\n",
    "# connect to index\n",
    "index = pc.Index(INDEX)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3f544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate embeddings\n",
    "BATCH_SIZE = 128  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(messages), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = messages[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response['data']):\n",
    "        assert i == be['index']  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e['embedding'] for e in response['data']]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({'text': messages, 'embedding': embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index)\n",
    "batch_size = 32  # process everything in batches of 32\n",
    "for i in tqdm(range(0, len(df['text']), batch_size)):\n",
    "    # set end position of batch\n",
    "    i_end = min(i+batch_size, len(df['text']))\n",
    "    # get batch of lines and IDs\n",
    "    lines_batch = df['text'][i:i+batch_size]\n",
    "    embeds = df['embedding'][i:i+batch_size]\n",
    "    ids_batch = ['training:' + str(n) for n in range(i, i_end)]\n",
    "    # prep metadata and upsert batch\n",
    "    meta = [{'text': line} for line in lines_batch]\n",
    "    to_upsert = zip(ids_batch, embeds, meta)\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=list(to_upsert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152ea4f-835b-48af-9faf-df00fbf44af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213f14b-8696-4912-b588-1e7d817c13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_embeddings(query: str = '', embeddings: dict = {}, max_tokens: int = 1024) -> str:\n",
    "    '''Return the max_tokens amount of related contexts based on the query string.\n",
    "\n",
    "    Keyword arguments:\n",
    "    query -- The query needing context\n",
    "    embeddings -- The list of embeddings and text\n",
    "    max_tokens -- The number of tokens to pull (Default 1024)\n",
    "\n",
    "    Returns:\n",
    "    The context of the query\n",
    "    '''\n",
    "    embedding = openai.Embedding.create(model=EMBEDDING_MODEL, input=query)['data'][0]['embedding']\n",
    "    context = [message for _, message in sorted(zip(cosine_similarity(embeddings['embedding'], [embedding]), embeddings['text']), reverse=True)]\n",
    "\n",
    "\n",
    "    max_tokens = int(max_tokens)\n",
    "\n",
    "    text = ''\n",
    "    total_tokens = 0\n",
    "    max_tokens = min(num_tokens('\\n'.join(context)) + total_tokens + 1, max_tokens + 1)\n",
    "    i = 0\n",
    "\n",
    "    for item in context:\n",
    "        nxt = f'\\n{item}\\n'\n",
    "        total_tokens += num_tokens(nxt)\n",
    "        if total_tokens > max_tokens:\n",
    "            break\n",
    "        text += nxt\n",
    "        i += 1\n",
    "\n",
    "    return f'{text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ac2b2-2e07-4ec4-a2fe-ad9ea1490257",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbox = ''\n",
    "while chatbox != 'exit':\n",
    "    chatbox = input()\n",
    "    \n",
    "    if chatbox == 'exit':\n",
    "        continue\n",
    "    \n",
    "    index = \"idx\"\n",
    "\n",
    "    # client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "    # STEP 1: Embed your prompt   (create an embedding from your prompt via OpenAI's embeddings)\n",
    "    embedding = openai.Embedding.create(model=EMBEDDING_MODEL, input=chatbox).data[0].embedding\n",
    "\n",
    "    # STEP 2: Initialize pinecone index\n",
    "    pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "\n",
    "    if index in pc.list_indexes().names():\n",
    "        idx = pc.Index(index)\n",
    "        result = idx.query(vector=[embedding], top_k=3, include_metadata=True)\n",
    "        # replace() added for the newlines\n",
    "        context = [x['metadata']['text'].replace('\\n', '') for x in result['matches']]\n",
    "\n",
    "\n",
    "        print(context) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
