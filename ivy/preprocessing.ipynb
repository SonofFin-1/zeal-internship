{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afdb8ce-1634-426a-8bdd-453ce8f40cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 0.28.0\n",
      "Uninstalling openai-0.28.0:\n",
      "  Successfully uninstalled openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from openai==0.28) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from openai==0.28) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from openai==0.28) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pdfminer in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (20191125)\n",
      "Requirement already satisfied: pycryptodome in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pdfminer) (3.20.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pdfminer.six in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (20231228)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pdfminer.six) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pdfminer.six) (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: docx2txt in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-pptx in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (0.6.23)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from python-pptx) (5.2.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /opt/homebrew/lib/python3.12/site-packages (from python-pptx) (10.3.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from python-pptx) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pinecone in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from pinecone) (2024.6.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pinecone) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pinecone) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pinecone-client in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (4.1.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from pinecone-client) (2024.6.2)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pinecone-client) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pinecone-client) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install packages\n",
    "%pip uninstall openai --yes\n",
    "%pip install openai==0.28\n",
    "%pip install tiktoken\n",
    "%pip install pdfminer\n",
    "%pip install pdfminer.six\n",
    "%pip install docx2txt\n",
    "%pip install python-pptx\n",
    "# %pip install pinecone-client==2.2.4\n",
    "%pip install pinecone\n",
    "%pip install pinecone-client --upgrade\n",
    "\n",
    "\n",
    "\n",
    "# imports\n",
    "import openai\n",
    "#from openai import OpenAI\n",
    "import os\n",
    "import tiktoken\n",
    "from tqdm.auto import tqdm     # this is our progress bar\n",
    "import pinecone\n",
    "import re\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text, extract_pages\n",
    "import docx2txt\n",
    "from pptx import Presentation\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "# constants\n",
    "GPT_MODEL = 'gpt-4o'\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "\n",
    "# this is for the input data\n",
    "PATH = './data'\n",
    "\n",
    "# these are Pinecone vars\n",
    "INDEX = 'idx'\n",
    "\n",
    "\n",
    "###### general info\n",
    "# the purpose of calculating the number of tokens is to ensure that the text chunks fit within the token limit of the model being used\n",
    "# The notebook splits the text into chunks based on a token limit of 256. \n",
    "    # so this is what decides how long each string in the res arr will be\n",
    "    # The choice of 256 tokens as the chunk size could be based on a balance between granularity and efficiency\n",
    "    # Smaller chunk sizes would result in more chunks and more embeddings to store, which could increase storage requirements and processing time\n",
    "    # Larger chunk sizes might lead to less specific embeddings and potentially miss important details within the text.\n",
    "# The size of each line depends on the original structure of the text files being processed.\n",
    "# The text-embedding-ada-002 model produces embeddings with a dimension of 1536, so the Pinecone index is created with the same dimension to ensure compatibility.\n",
    "# The batch size is set to 128 for generating embeddings using the OpenAI API.\n",
    "    # The batch size is set to 32 when uploading the embeddings and metadata to Pinecone using the upsert operation.\n",
    "    # Larger batch sizes can be more efficient in terms of API calls and processing time, but they also require more \n",
    "    # memory and may be limited by the API's constraints\n",
    "# The max_tokens parameter determines the maximum number of tokens to retrieve from the relevant chunks based on the input query.\n",
    "    # Retrieving more tokens provides more context but also increases the response size and processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff33871a-c7d5-4ddf-886b-ff3cc119cea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ENV = os.getenv(\"ENV\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00b1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = 'gpt-4') -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2adb25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af786385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    return extract_text(path).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823e649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(path):\n",
    "    # extract text\n",
    "    return docx2txt.process(path).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf5bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pptx(path):\n",
    "    split = []\n",
    "    prs = Presentation(path)\n",
    "    print(\"----------------------\")\n",
    "    temp = ''\n",
    "    enterCount = 0\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, 'text'):\n",
    "#                 if len(shape.text) > 0:\n",
    "#                     temp += shape.text\n",
    "#                 elif enterCount > 1:\n",
    "#                     split.append(temp)\n",
    "#                     temp = ''\n",
    "#                     enterCount = 0\n",
    "#                 elif len(shape.text) == 0:\n",
    "#                     enterCount += 1\n",
    "                split.append(shape.text)\n",
    "    \n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04189e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xlsx(path):\n",
    "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(PATH)\n",
    "    \n",
    "    xml_file = os.path.join(PATH, 'xl/sharedStrings.xml')\n",
    "    \n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for elem in root.iter():\n",
    "        if elem.text:\n",
    "            result.append(elem.text.strip())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82080044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_png(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        \n",
    "        openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "        \n",
    "        messages = [\n",
    "            { 'role': 'system', 'content': 'system message here' },\n",
    "            { 'role': 'user', 'content': [\n",
    "                {'type': 'text', 'text': f'Convert the following image into a text table.'},\n",
    "                {'type': 'image_url', 'image_url': { 'url': f'data:image/png;base64,{image}'} }\n",
    "            ]}\n",
    "        ]\n",
    "        \n",
    "        functions = [\n",
    "            {\n",
    "                'name': 'write_queries',\n",
    "                'description': 'Get information from the database',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'details': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The information needed from the database',\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        result = ''\n",
    "        \n",
    "        for chunk in openai.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "            temperature=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            function_call=None,\n",
    "            stream=True\n",
    "        ):\n",
    "            if len(chunk.choices) > 0:\n",
    "                response = chunk.choices[0]\n",
    "                finish_reason = response.finish_reason\n",
    "                content = response.delta.content\n",
    "                function = response.delta.function_call\n",
    "\n",
    "                if content is not None:\n",
    "                    print(content, end='')\n",
    "                    result += content\n",
    "        \n",
    "        return result.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f888cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_files(root_path):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            all_files.append(file_path)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88364573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range(s, delimiter = '.'):\n",
    "    parts = s.split(delimiter)\n",
    "    return delimiter.join(parts[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7859f9d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/shape-up.pdf\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "files = collect_all_files(PATH)\n",
    "\n",
    "lines = []\n",
    "for file in files:\n",
    "    lines = []\n",
    "    print(file)\n",
    "    suffix = get_range(file)\n",
    "    if suffix == 'txt':\n",
    "        lines = read_txt(file)\n",
    "    elif suffix == 'pdf':\n",
    "        lines = read_pdf(file)\n",
    "    elif suffix == 'docx':\n",
    "        lines = read_docx(file)\n",
    "    elif suffix == 'pptx':\n",
    "        lines = read_pptx(file)\n",
    "    elif suffix == 'xlsx':\n",
    "        lines = read_xlsx(file)\n",
    "    elif suffix == 'png':\n",
    "        lines = read_png(file)\n",
    "\n",
    "    temp_string = []\n",
    "    # print(lines)\n",
    "    for line in lines:\n",
    "        if num_tokens('\\n'.join(temp_string)) > 256:\n",
    "            # print('\\n'.join(temp_string))\n",
    "            messages.append('\\n'.join(temp_string))\n",
    "            temp_string = []\n",
    "        else:\n",
    "            temp_string.append(line)\n",
    "    if num_tokens('\\n'.join(temp_string)) < 256:\n",
    "        # print('\\n'.join(temp_string))\n",
    "        messages.append('\\n'.join(temp_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470b5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [message for message in messages if len(message) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c5fd5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not messages: \n",
    "    # test out messages\n",
    "    messages = [\"this is message 1\", \"this is message 2\", \"this is message 3\", \"this is message 4\", \"meow meow\", \"woof woof\", \"I am a cat\"]\n",
    "\n",
    "# print(messages)\n",
    "# for i in range(len(messages)):\n",
    "#     print(len(messages[i]))\n",
    "#     print(messages[i])\n",
    "#     print(\" \\n###########################\\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b013bc-e1c8-4fd8-ae89-15c0f6966eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pinecone.data.index.Index object at 0x10d052c60>\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, PodSpec\n",
    "\n",
    "INDEX = 'idx'\n",
    "\n",
    "# pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "# print(f\"{os.environ['PINECONE_API_KEY']}\")\n",
    "\n",
    "if INDEX not in pc.list_indexes().names():\n",
    "    pc.create_index(INDEX, dimension=1536, metric='cosine', spec=PodSpec(environment=ENV))\n",
    "# connect to index\n",
    "index = pc.Index(INDEX)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab3f544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 127\n",
      "Batch 128 to 255\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings\n",
    "BATCH_SIZE = 128  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(messages), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = messages[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response['data']):\n",
    "        assert i == be['index']  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e['embedding'] for e in response['data']]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({'text': messages, 'embedding': embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64f7bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pinecone.data.index.Index object at 0x10d052c60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "print(index)\n",
    "batch_size = 32  # process everything in batches of 32\n",
    "for i in tqdm(range(0, len(df['text']), batch_size)):\n",
    "    # set end position of batch\n",
    "    i_end = min(i+batch_size, len(df['text']))\n",
    "    # get batch of lines and IDs\n",
    "    lines_batch = df['text'][i:i+batch_size]\n",
    "    embeds = df['embedding'][i:i+batch_size]\n",
    "    ids_batch = ['training:' + str(n) for n in range(i, i_end)]\n",
    "    # prep metadata and upsert batch\n",
    "    meta = [{'text': line} for line in lines_batch]\n",
    "    to_upsert = zip(ids_batch, embeds, meta)\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=list(to_upsert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3152ea4f-835b-48af-9faf-df00fbf44af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9213f14b-8696-4912-b588-1e7d817c13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_embeddings(query: str = '', embeddings: dict = {}, max_tokens: int = 1024) -> str:\n",
    "    '''Return the max_tokens amount of related contexts based on the query string.\n",
    "\n",
    "    Keyword arguments:\n",
    "    query -- The query needing context\n",
    "    embeddings -- The list of embeddings and text\n",
    "    max_tokens -- The number of tokens to pull (Default 1024)\n",
    "\n",
    "    Returns:\n",
    "    The context of the query\n",
    "    '''\n",
    "    embedding = openai.Embedding.create(model=EMBEDDING_MODEL, input=query)['data'][0]['embedding']\n",
    "    context = [message for _, message in sorted(zip(cosine_similarity(embeddings['embedding'], [embedding]), embeddings['text']), reverse=True)]\n",
    "\n",
    "\n",
    "    max_tokens = int(max_tokens)\n",
    "\n",
    "    text = ''\n",
    "    total_tokens = 0\n",
    "    max_tokens = min(num_tokens('\\n'.join(context)) + total_tokens + 1, max_tokens + 1)\n",
    "    i = 0\n",
    "\n",
    "    for item in context:\n",
    "        nxt = f'\\n{item}\\n'\n",
    "        total_tokens += num_tokens(nxt)\n",
    "        if total_tokens > max_tokens:\n",
    "            break\n",
    "        text += nxt\n",
    "        i += 1\n",
    "\n",
    "    return f'{text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "588ac2b2-2e07-4ec4-a2fe-ad9ea1490257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbox = ''\n",
    "# while chatbox != 'exit':\n",
    "#     chatbox = input()\n",
    "    \n",
    "#     if chatbox == 'exit':\n",
    "#         continue\n",
    "    \n",
    "#     index = \"idx\"\n",
    "\n",
    "#     # client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "#     # STEP 1: Embed your prompt   (create an embedding from your prompt via OpenAI's embeddings)\n",
    "#     embedding = openai.Embedding.create(model=EMBEDDING_MODEL, input=chatbox).data[0].embedding\n",
    "\n",
    "#     # STEP 2: Initialize pinecone index\n",
    "#     pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "\n",
    "#     if index in pc.list_indexes().names():\n",
    "#         idx = pc.Index(index)\n",
    "#         result = idx.query(vector=[embedding], top_k=3, include_metadata=True)\n",
    "#         # replace() added for the newlines\n",
    "#         context = [x['metadata']['text'].replace('\\n', '') for x in result['matches']]\n",
    "\n",
    "\n",
    "#         print(context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c4d2b1e-f757-4b80-9821-cc73eb41d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " exit\n"
     ]
    }
   ],
   "source": [
    "chatbox = ''\n",
    "while chatbox != 'exit':\n",
    "    chatbox = input()    \n",
    "    if chatbox == 'exit':\n",
    "        continue    \n",
    "    index = \"idx\"    \n",
    "    \n",
    "    # if using newer version of openAI import:    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])    \n",
    "    # STEP 1: Embed your prompt\n",
    "    embedding = openai.Embedding.create(model=EMBEDDING_MODEL, input=chatbox).data[0].embedding    \n",
    "\n",
    "    print(\"embedding generated successfully\")\n",
    "    \n",
    "    # STEP 2: Initialize pinecone index\n",
    "    pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])    \n",
    "\n",
    "\n",
    "    ### increase top_k to increase the num of vectors used from PineCone (so top_k=3 would only use the top 3 similar vectors)\n",
    "    if index in pc.list_indexes().names():\n",
    "        idx = pc.Index(index)\n",
    "        result = idx.query(vector=[embedding], top_k=1024, include_metadata=True)\n",
    "        context = [x['metadata']['text'].replace('\\n', '') for x in result['matches']]\n",
    "        #print(context)\n",
    "\n",
    "\n",
    "    # limit the num of max_tokens given to the chat completion bot to openAI's limit of 8192\n",
    "    # this just cuts off any text that goes beyond the limit, so we lose all of that context\n",
    "    max_tokens = 8192\n",
    "    text = ''\n",
    "    total_tokens = 0\n",
    "    max_tokens = min(num_tokens('\\n'.join(context)) + total_tokens + 1, max_tokens + 1)\n",
    "    i = 0\n",
    "\n",
    "    for item in context:\n",
    "        nxt = f'\\n{item}\\n'\n",
    "        total_tokens += num_tokens(nxt)\n",
    "        if total_tokens > max_tokens:\n",
    "            break\n",
    "        text += nxt\n",
    "        i += 1\n",
    "\n",
    "    # return f'{text}'\n",
    "    print(f\"length of chunked text: {len(text)}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # STEP 3: Generate a response using the following pre-exisiting context from Pinecone\n",
    "    messages = [\n",
    "        # This message sets the behavior and tone of the assistant. By specifying the role as system, it defines \n",
    "        # an instruction or guideline for the AI's behavior throughout the conversation. Here, it instructs the \n",
    "        # AI to act as a helpful assistant\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        # this is what the user types into the chat\n",
    "        {\"role\": \"user\", \"content\": chatbox},\n",
    "        # This message acts as a preamble for the context that will be provided next. It sets up the expectation \n",
    "        # that the following content will be the context based on which the assistant should generate a response. \n",
    "        # The role is assistant, which might be less common but can be used to shape the conversation dynamically\n",
    "        {\"role\": \"assistant\", \"content\": \"Based on the following context:\"},\n",
    "        # This message provides the actual context retrieved from Pinecone. The context variable is a list of text snippets.\n",
    "        # The \" \".join(context) part concatenates all the text snippets into a single string, separated by spaces. \n",
    "        # The role is system, indicating that this is background information or context for the assistant to use when generating a response.\n",
    "        {\"role\": \"system\", \"content\": \" \".join(text)}\n",
    "    ]    \n",
    "\n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\", \"content\": \"You are William Shakespeare. Keep sentences summarized to less than 20 words. If asked to explain, give full detail. Only respond using the context provided. Do not add any extra information. For any unrelated questions, tell the user to ask something about the context.\"},\n",
    "    #     {\"role\": \"user\", \"content\": chatbox},\n",
    "    #     {\"role\": \"system\", \"content\": \" \".join(context)}\n",
    "    #     #{\"role\": \"user\", \"content\": chatbox},\n",
    "    # ]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        # more tokens means longer/more-detailed responses and VV\n",
    "        # this is the num of tokens in the chat completion's response, but not the num of tokens input into the chatbot\n",
    "        max_tokens=1500,\n",
    "        # more temp is a more subjective/creative/random response, less temp is more objective/direct/deterministic response\n",
    "        temperature=0.2\n",
    "    )    \n",
    "    \n",
    "    # Extract and print the response text\n",
    "    response_text = response.choices[0].message['content'].strip()\n",
    "    print(f\"\\n\\nWelcome to Costco, I love you:\\n\\n {response_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
